// Targeted by JavaCPP version 1.5.8-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.tensorflowlite;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.tensorflowlite.global.tensorflowlite.*;
  // namespace interpreter_wrapper

/** Options class for {@code Interpreter}.
 *  WARNING: This is an experimental API and subject to change. */
@Namespace("tflite") @NoOffset @Properties(inherit = org.bytedeco.tensorflowlite.presets.tensorflowlite.class)
public class InterpreterOptions extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public InterpreterOptions(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public InterpreterOptions(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public InterpreterOptions position(long position) {
        return (InterpreterOptions)super.position(position);
    }
    @Override public InterpreterOptions getPointer(long i) {
        return new InterpreterOptions((Pointer)this).offsetAddress(i);
    }

  public InterpreterOptions() { super((Pointer)null); allocate(); }
  private native void allocate();

  /** Preserving all intermediates tensors for debugging.
   *  WARNING: This is an experimental API and subject to change. */
  public native void SetPreserveAllTensors(@Cast("bool") boolean value/*=true*/);
  public native void SetPreserveAllTensors();

  /** Returns if the {@code experimental_preserve_all_tensors_} feature is enabled.
   *  WARNING: This is an experimental API and subject to change. */
  public native @Cast("bool") boolean GetPreserveAllTensors();

  /** Force all intermediate dynamic tensors to be released once they are not
   *  used by the model. Please use this configuration with caution, since it
   *  might reduce the peak memory usage of the model at the cost of a slower
   *  inference speed.
   *  WARNING: This is an experimental API and subject to change. */
  public native void SetEnsureDynamicTensorsAreReleased(@Cast("bool") boolean value/*=true*/);
  public native void SetEnsureDynamicTensorsAreReleased();

  /** Returns if the {@code experimental_ensure_dynamic_tensors_are_released_} feature
   *  is enabled.
   *  WARNING: This is an experimental API and subject to change. */
  public native @Cast("bool") boolean GetEnsureDynamicTensorsAreReleased();

  /** Use dynamic tensor allocation method for large tensors instead of static
   *  memory planner. It improves peak memory usage but there could be some
   *  latency impact. The value is used to determine large tensors.
   *  WARNING: This is an experimental API and subject to change. */
  public native void SetDynamicAllocationForLargeTensors(int value);

  /** Returns the size threshold for dynamic tensor allocation method.
   *  It returns zero if the feature is not enabled.
   *  WARNING: This is an experimental API and subject to change. */
  public native int GetDynamicAllocationForLargeTensors();
}
